ggtitle(paste(sep="",i," Realisation of  UM" ))
d=ggplot()+
geom_raster(data=sim_ds,aes(x=X, y=Y,fill=sim_ds[,2+i+300]))+
coord_fixed()+
theme_classic()+
scico::scale_fill_scico(palette = "bilbao")+
ggtitle(paste(sep="",i," Realisation of SA " ))
gridExtra::grid.arrange(a, b,c,d,nrow=2,ncol=2) #set the number of plots
}
manipulate(all_lcode_simulationplot(i,simlcodes), i=slider(1,100))
save.image("U:/STUDY/blockkurs/2022/Ibadullaev/fu.RData")
save.image("U:/STUDY/blockkurs/2022/Ibadullaev/fu.RData")
save.image("U:/STUDY/blockkurs/2022/Ibadullaev/fu.RData")
## Lcode + Co
(mat = matrix(c(1,2,1,3), ncol=2))
layout(mat, height=c(2,3))  # alternative to par(mfrow=...)
(lvl = abbreviate(levels(ds$Lcode))) # useful later
# rock type vs logCo
plot(log(Co)~Lcode, data=ds, border=1:5)
## Apparently, Co is lowest in FZ, lower in UM,
#  average in SM  and highest in
#  SA. There is a dependence between CO and lithology!
#  Given these results +  low number of data in Portlandian:
#convert categorical values to integers
ds$lithcode =
ds$Lcode %>% # take the original Lcode
as.integer %>%   # convert the categories to integers (==position in the lvl vector)
factor(labels=c("FZ", "SA", "SM", "UM"))
# check the operation was well done
summary(ds$lithcode)
table(ds$lithcode, ds$Lcode)
contrasts(ds$lithcode) <- contr.treatment(levels(ds$lithcode), base=2)
# check the effect: (if necessary, read ?contr.treatment)
ds$lithcode
contr.treatment(levels(ds$lithcode), base=2) %>%
{lm(log(Co)~lithcode, data=ds, contrasts=.)} %>% summary
## multivariate model for Co and lcode together
gs_Co4lcodes = gstat(id="Co", formula=log(Co)~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="FZ", formula=(lithcode=="FZ")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="SA", formula=(lithcode=="SA")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="UM", formula=(lithcode=="UM")~1, locations = ~X+Y,
data=ds, nmax=50)
par(mfrow=c(1,1))
# empirical variogram
vg_Co4lcodes = variogram(gs_Co4lcodes, cutoff=225)
plot(vg_Co4lcodes)
# variogram model
# let's start with the model for Ni, without nugget
# adding the wave model of Tuesday
vgt = vgm(model="Sph", range=100,nugget = 0.2 ,psill=0.05)
# you can also try switching in and out some of the components and slightly move ranges
gs_Co4lcodes = gstat::fit.lmc(v=vg_Co4lcodes, model=vgt, g=gs_Co4lcodes, correct.diagonal = 1.0001)
plot(vg_Co4lcodes, model=gs_Co4lcodes$model)
# vectors of x and y coordinates of the grid nodes,
#   covering the range of the data, and with a step
#   allowing for interpolation between data
par(mfrow=c(1,1))
plot(Y~X, data=ds, asp=1)
xx = seq(from=rangesXY[1,"X"], to=rangesXY[2,"X"],by=10, length.out = length(ds$X))
## multivariate model for Co and lcode together
gs_Co4lcodes = gstat(id="Co", formula=log(Co)~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="FZ", formula=(lithcode=="FZ")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="SA", formula=(lithcode=="SA")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="UM", formula=(lithcode=="UM")~1, locations = ~X+Y,
data=ds, nmax=50)
par(mfrow=c(1,1))
# empirical variogram
vg_Co4lcodes = variogram(gs_Co4lcodes, cutoff=225)
plot(vg_Co4lcodes)
# variogram model
# let's start with the model for Ni, without nugget
# adding the wave model of Tuesday
vgt = vgm(model="Sph", range=100,nugget = 0.2 ,psill=0.05)
# you can also try switching in and out some of the components and slightly move ranges
gs_Co4lcodes = gstat::fit.lmc(v=vg_Co4lcodes, model=vgt, g=gs_Co4lcodes, correct.diagonal = 1.0001)
plot(vg_Co4lcodes, model=gs_Co4lcodes$model)
# vectors of x and y coordinates of the grid nodes,
#   covering the range of the data, and with a step
#   allowing for interpolation between data
par(mfrow=c(1,1))
plot(Y~X, data=ds, asp=1)
xx = seq(from=rangesXY[1,"X"], to=rangesXY[2,"X"], length.out = length(ds$X))
yy = seq(from=rangesXY[1,"Y"], to=rangesXY[2,"Y"], length.out = length(ds$X))
# 4 points
# grid definition must be the same in gs object Easting=x, Northing=y
xxyy_grid = expand.grid(X=xx, Y=yy)
points(xxyy_grid,col=2,pch=".")
xv_Co4lcodes = gstat.cv(gs_Co4lcodes)
## multivariate model for Co and lcode together
gs_Co4lcodes = gstat(id="Co", formula=log(Co)~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="FZ", formula=(lithcode=="FZ")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="SA", formula=(lithcode=="SA")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="UM", formula=(lithcode=="UM")~1, locations = ~X+Y,
data=ds, nmax=50)
par(mfrow=c(1,1))
# empirical variogram
vg_Co4lcodes = variogram(gs_Co4lcodes, cutoff=225)
plot(vg_Co4lcodes)
# variogram model
# let's start with the model for Ni, without nugget
# adding the wave model of Tuesday
vgt = vgm(model="Sph", range=100,nugget = 0.2 ,psill=0.05)
# you can also try switching in and out some of the components and slightly move ranges
gs_Co4lcodes = gstat::fit.lmc(v=vg_Co4lcodes, model=vgt, g=gs_Co4lcodes, correct.diagonal = 1.0001)
plot(vg_Co4lcodes, model=gs_Co4lcodes$model)
# vectors of x and y coordinates of the grid nodes,
#   covering the range of the data, and with a step
#   allowing for interpolation between data
par(mfrow=c(1,1))
plot(Y~X, data=ds, asp=1)
xx = seq(from=rangesXY[1,"X"], to=rangesXY[2,"X"], length.out = length(ds$X))
yy = seq(from=rangesXY[1,"Y"], to=rangesXY[2,"Y"], length.out = length(ds$X))
# 4 points
# grid definition must be the same in gs object Easting=x, Northing=y
xxyy_grid = expand.grid(X=xx, Y=yy)
points(xxyy_grid,col=2,pch=".")
xv_Co4lcodes = gstat.cv(gs_Co4lcodes)
# cokriging
cok_Co4lcodes = predict(gs_Co4lcodes, newdata=xxyy_grid, debug.level = -1)
myplot = function(x, variable, breaks=10, colorscale=RColorBrewer::brewer.pal(11, "Spectral")){
# allow for giving specific breaks or the desired number of breaks
if(length(breaks)==1){
breaks = pretty(x[,variable], n = breaks)
}
# ensure that the color scale has always one color less than breaks
if(length(breaks)-length(colorscale)!=1){
colorscale = colorRampPalette(colorscale)(length(breaks)-1)
}
# plot
cols = colorscale[cut(as.numeric(x[,variable]), breaks = breaks)]
plot(Y~X, data=x, bg=cols, col=NA, asp=1, pch=22)
invisible(list(breaks=breaks, color=colorscale)) # return invisibly the elements of the legend
}
par(mfrow=c(3,3))
#1
myplot(cok_Co4lcodes, variable = "Co.pred");
title("Co.pred")
#2
myplot(cok_Co4lcodes, variable = "FZ.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("FZ.pred")
myplot(cok_Co4lcodes, variable = "SA.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("SA.pred")
myplot(cok_Co4lcodes, variable = "UM.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("UM.pred")
#red is negative or zero, orange 0 and 0.25, yellow transition zone, blue , 1 and 2 dark blue-the highest prob
#3
cok_Co4lcodes$SM.pred = 1 -cok_Co4lcodes$FZ.pred -cok_Co4lcodes$SA.pred-cok_Co4lcodes$UM.pred
myplot(cok_Co4lcodes, variable = "SM.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("SM.pred")
#4
mostProb=cok_Co4lcodes[,c(5,7,9,17)] %>% apply(1, which.max)
cok_Co4lcodes$Lcode.pred =mostProb
myplot(cok_Co4lcodes, variable = "Lcode.pred",breaks =c(0,1.5,2.5,3.5,4.5),col=1:4);
title("The most probable")
myplot(ds, variable = "Lcode",breaks =c(0,1.5,2.5,3.5,4.5),col=1:4);
title("The truth")
# for Ni
## multivariate model for Ni and lcodes together
gs_Ni4lcodes = gstat(id="Ni", formula=log(Ni)~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="FZ", formula=(lithcode=="FZ")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="SA", formula=(lithcode=="SA")~1, locations = ~X+Y,
data=ds, nmax=50) %>%
gstat(id="UM", formula=(lithcode=="UM")~1, locations = ~X+Y,
data=ds, nmax=50)
par(mfrow=c(1,1))
# empirical variogram
vg_Ni4lcodes = variogram(gs_Ni4lcodes, cutoff=225)
plot(vg_Ni4lcodes)
# variogram model
# let's start with the model for Ni, without nugget
# adding the wave model of Tuesday
vgt = vgm(model="Sph", range=100,nugget = 0.2 ,psill=0.05)
# you can also try switching in and out some of the components and slightly move ranges
gs_Ni4lcodes = gstat::fit.lmc(v=vg_Ni4lcodes, model=vgt, g=gs_Ni4lcodes, correct.diagonal = 1.0001)
plot(vg_Ni4lcodes, model=gs_Ni4lcodes$model)
# vectors of x and y coordinates of the grid nodes,
#   covering the range of the data, and with a step
#   allowing for interpolation between data
par(mfrow=c(1,1))
plot(Y~X, data=ds, asp=1)
xx = seq(from=rangesXY[1,"X"], to=rangesXY[2,"X"], length.out = length(ds$X))
yy = seq(from=rangesXY[1,"Y"], to=rangesXY[2,"Y"], length.out = length(ds$X))
# 4 points
# grid definition must be the same in gs object Easting=x, Northing=y
xxyy_grid = expand.grid(X=xx, Y=yy)
points(xxyy_grid,col=2,pch=".")
xv_Ni4lcodes = gstat.cv(gs_Ni4lcodes)
# cokriging
cok_Ni4lcodes = predict(gs_Ni4lcodes, newdata=xxyy_grid, debug.level = -1)
myplot = function(x, variable, breaks=10, colorscale=RColorBrewer::brewer.pal(11, "Spectral")){
# allow for giving specific breaks or the desired number of breaks
if(length(breaks)==1){
breaks = pretty(x[,variable], n = breaks)
}
# ensure that the color scale has always one color less than breaks
if(length(breaks)-length(colorscale)!=1){
colorscale = colorRampPalette(colorscale)(length(breaks)-1)
}
# plot
cols = colorscale[cut(as.numeric(x[,variable]), breaks = breaks)]
plot(Y~X, data=x, bg=cols, col=NA, asp=1, pch=22)
invisible(list(breaks=breaks, color=colorscale)) # return invisibly the elements of the legend
}
par(mfrow=c(3,3))
#1
myplot(cok_Ni4lcodes, variable = "Ni.pred");
title("Ni.pred")
#2
myplot(cok_Ni4lcodes, variable = "FZ.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("FZ.pred")
myplot(cok_Ni4lcodes, variable = "SA.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("SA.pred")
myplot(cok_Ni4lcodes, variable = "UM.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("UM.pred")
#red is negative or zero, orange 0 and 0.25, yellow transition zone, blue , 1 and 2 dark blue-the highest prob
#3
cok_Ni4lcodes$SM.pred = 1 -cok_Ni4lcodes$FZ.pred -cok_Ni4lcodes$SA.pred-cok_Ni4lcodes$UM.pred
myplot(cok_Ni4lcodes, variable = "SM.pred",breaks =c(-2,0,0.25,0.5,1,2));
title("SM.pred")
#4
mostProb=cok_Ni4lcodes[,c(5,7,9,17)] %>% apply(1, which.max)
cok_Ni4lcodes$Lcode.pred =mostProb
myplot(cok_Ni4lcodes, variable = "Lcode.pred",breaks =c(0,1.5,2.5,3.5,4.5),col=1:4);
title("The most probable")
myplot(ds, variable = "Lcode",breaks =c(0,1.5,2.5,3.5,4.5),col=1:4);
title("The truth")
legend("bottomright",legend = levels(ds$Lcode), col = 1:4)
save.image("U:/STUDY/blockkurs/2022/Ibadullaev/fu.RData")
ds = read.csv("Ibadullaev.csv",stringsAsFactors = T) # load data
colnames(ds) # check cols
ds = ds[,-1] # clip 1st col
colnames(ds) # check cols
ds = read.csv("Ibadullaev.csv",stringsAsFactors = T) # load data
colnames(ds) # check cols
ds = ds[,-1] # clip 1st col
colnames(ds) # check cols
# Import Packages
library("gstat")
library("RColorBrewer")
library("magrittr")
library("ggplot2")
library("manipulate")
library("sp")
library("scico")
library("dplyr")
library("gmGeostats")
library("knitr")
library("rstudioapi")
setwd(dirname(getActiveDocumentContext()$path)) # Set active directory
getwd()
```{r showsimlcodes}
all_lcode_simulationplot(i=30,simlcodes)
allsimulationplot(15,cosim_l,n=20)
allsimulationplot(15,cosim_l,n=20)
#manipulate(simulationplot(i,sim_Co,varname="Co"), i=slider(1,100))
simulationplot(i=30,sim_Co,varname="Co")
#manipulate(simulationplot(i,sim_Ni,varname="Ni"), i=slider(1,100))
simulationplot(i=25,sim_Ni,varname="Ni")
source('U:/STUDY/R_remote_sensing/Tutorial1_SingleDate_ImageClassification/image_classification4chap.R', echo=TRUE)
featurePlot(x = training[, 2:7],
y = training$Cl1984,
plot = 'density',
labels=c("Reflectance", "Density distribution"),
scales = list(x = list(relation="free"),
y = list(relation="free")),
layout = c(3, 2),
auto.key = list(columns = 3))
featurePlot(x = training[, 2:7],
y = as.character(training$Cl1984),
plot = 'density',
labels=c("Reflectance", "Density distribution"),
scales = list(x = list(relation="free"),
y = list(relation="free")),
layout = c(3, 2),
auto.key = list(columns = 3))
ta_data@data = na.omit(ta_data@data)
ta_data == NA
complete.cases(ta_data@data)
is.na(ta_data@data)
?na.omit()
featurePlot(x = training[, 2:7],
y = as.character(training$Cl1984),
plot = 'density',
labels=c("Reflectance", "Density distribution"),
scales = list(x = list(relation="free"),
y = list(relation="free")),
layout = c(3, 2),
auto.key = list(columns = 3))
featurePlot(x = training[, 2:7],
y = as.factor(training$Cl1984),
plot = 'density',
labels=c("Reflectance", "Density distribution"),
scales = list(x = list(relation="free"),
y = list(relation="free")),
layout = c(3, 2),
auto.key = list(columns = 3))
skewnessValues = apply(training[, 2:7],2,skewness)
skewnessValues = apply(training[, 2:7],2,skewness())
# Clearly, the density plots show that bands 1 and 2 (that is
#                                                     “CostzPr_22_6_84.1” and “CostzPr_22_6_84.2”) display significant
# skewness. We can also calculate skewness of the bands using the
# skewness() function available in the e1071 package
library(e1071)
skewnessValues = apply(training[, 2:7],2,skewness())
skewnessValues = apply(x=training[, 2:7],2,FUN=skewness())
skewnessValues = apply(x=training[, 2:7],2,FUN = skewness)
skewnessValues = apply(x=training[, 2:7],MARGIN = 2,FUN = skewness)
skewnessValues <- apply(training[, 2:7], 2, skewness)
skewnessValues <- apply(x=training[, 2:7],2,FUN = skewness)
skewnessValues <- apply(X=training[, 2:7],2, skewness)
skewnessValues <- apply(training[, 2:7], 2, skewness)
skewnessValues
featurePlot(x=training[,2:7],y=training$Cl1984,
plot="box",
scales=list(y=list(relation="free"),
x=list(rot=90)),
layout=c(2,3),
auto.key=list(colmns=2))
featurePlot(x=training[,2:7],y=as.factor(training$Cl1984),
plot="box",
scales=list(y=list(relation="free"),
x=list(rot=90)),
layout=c(2,3),
auto.key=list(colmns=2))
#check relationships between 2 bands
Band1_2<- ggplot(data = ta_data@data, aes(Layer_1, Layer_2)) + geom_point(aes(shape= Cl1984, colour= Cl1984)) + labs(x="Band 1", y = "Band 2")
Band1_3<-ggplot(data = ta_data@data, aes(Layer_1, Layer_3)) + geom_point(aes(shape= Cl1984, colour= Cl1984)) + labs(x="Band 1", y = "Band 3")
Band1_4<-ggplot(data = ta_data@data, aes(Layer_1, Layer_4)) + geom_point(aes(shape= Cl1984, colour= Cl1984)) + labs(x="Band 1", y = "Band 4")
Band1_5<- ggplot(data = ta_data@data, aes(Layer_1, Layer_5)) + geom_point(aes(shape= Cl1984, colour= Cl1984)) + labs(x="Band 1", y = "Band 5")
Band1_7<-ggplot(data = ta_data@data, aes(Layer_1, Layer_6)) + geom_point(aes(shape= Cl1984, colour= Cl1984)) + labs(x="Band 1", y = "Band 7")
grid.arrange(Band1_2, Band1_3,Band1_4,Band1_5,Band1_7)
bandCorrelations =cor(training[,2:7])
dim(bandCorrelations)
bandCorrelations %>% table
library(magrittr)
bandCorrelations %>% table
bandCorrelations =cor(training[,2:7])
dim(bandCorrelations)
library(magrittr)
bandCorrelations %>% table
bandCorrelations
bandCorrelations %>% table()
bandCorrelations
# plot correlations
corrplot(bandCorrelations, method = "number",
type = "upper")
corrplot(bandCorrelations, method = "color",
order = "hclust",type = "lower")
corrplot.mixed(bandCorrelations,lower.col = "black",
number.cex=.7, upper = "color")
fitControl = trainControl(method = "repeatedcv",
number = 5,
repeats = 5)
set.seed(hre_seed)
knnFit =train(Cl1984~., data=training,
method="kknn",
preProcess=c("center","scale"),
trControl=fitControl)
#check model results
print(knnFit)
#trining perfomance
plot(knnFit)
#check parameters of the best model
knnFit$finalModel
# display variable importance using the varImp() function
knn_varImp = varImp(knnFit,compete=F)
?varImp
# display variable importance using the varImp() function
knn_varImp = varImp(knnFit,competes=F)
# display variable importance using the varImp() function
knn_varImp = varImp(knnFit,competes=FALSE)
# display variable importance using the varImp() function
knn_varImp = varImp(knnFit)
# display variable importance using the varImp() function
knn_varImp = varImp(object = knnFit,competes=FALSE)
# display variable importance using the varImp() function
knn_varImp = varImp(knnFit, compete = FALSE)
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit, compete = FALSE)
#check parameters of the best model
knnFit$finalModel
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit, competes = FALSE)
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit)
knnFit =caret::train(as.factor(Cl1984)~., data=training,
method="kknn",
preProcess=c("center","scale"),
trControl=fitControl)
#check model results
print(knnFit)
#training perfomance
plot(knnFit)
#check parameters of the best model
knnFit$finalModel
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit)
knnFit =caret::train(factor(Cl1984)~., data=training,
method="kknn",
preProcess=c("center","scale"),
trControl=fitControl)
#check model results
print(knnFit)
#training perfomance
plot(knnFit)
#check parameters of the best model
knnFit$finalModel
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit)
head(training)
training$Cl1984=factor(training$Cl1984)
knnFit =caret::train(Cl1984~., data=training,
method="kknn",
preProcess=c("center","scale"),
trControl=fitControl)
#check model results
print(knnFit)
#training perfomance
plot(knnFit)
#check parameters of the best model
knnFit$finalModel
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit)
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit, competes=F)
knn_varImp
#training perfomance
plot(knnFit)
fitControl = trainControl(method = "repeatedcv",
number = 5,
repeats = 5)
set.seed(hre_seed)
training$Cl1984=factor(training$Cl1984)
knnFit =caret::train(Cl1984~., data=training,
method="kknn",
preProcess=c("center","scale"),
trControl=fitControl)
#check model results
print(knnFit)
#training perfomance
plot(knnFit)
#check parameters of the best model
knnFit$finalModel
# display variable importance using the varImp() function
knn_varImp = caret::varImp(knnFit, competes=F)
knn_varImp
plot(knn_varImp)
# now let's predict
predKnn = predict(knnFit,newdata=testing)
# Next, let’s check the accuracy metrics for the KNN model using the
# confusionMatrix() function from the caret package in R.
confusionMatrix(data=predKnn, testing$Cl1984 )
predKnn
# Next, let’s check the accuracy metrics for the KNN model using the
# confusionMatrix() function from the caret package in R.
confusionMatrix(data=factor(predKnn), testing$Cl1984 )
# Next, let’s check the accuracy metrics for the KNN model using the
# confusionMatrix() function from the caret package in R.
confusionMatrix(data=predKnn, factor(testing$Cl1984) )
# ANN CLASSIFIER----------
set.seed(hre_seed)
training$Cl1984=factor(training$Cl1984)
annFit = train(Cl1984 ~., data = training,
method="nnet",
preProcess = c("center","scale"),
trControl =fitControl)
print(annFit)
plot(annFit)
# parameters of the best model
annFit$finalModel
#we can plot the net
plotnet(annFit$finalModel)
# importance of variables
olden(annFit)
predAnn = predict(annFit, newdata=testing)
confusionMatrix(predAnn, factor(testing$Cl1984))
In general, the confusion matrix shows high misclassification
errors. The overall accuracy is relatively poor. Generally, the KNN
classifier has better accuracy than the ANN classifier.
In terms of the producer’s accuracy, sensitivity for the agriculture
(agr) class is 50%, while the specificity is 82%. This means that the
ANN classifier correctly identified agriculture (Agr) 50% of the time,
while 82% of the time, the ANN classifier correctly identified
non-agriculture (non-agr) classes (that is bareland, green spaces,
settlement and water). However, the sensitivity and specificity for the
bareland is much closer. It also interesting to note that sensitivity is
lower than specificity for the green space class. The positive predicted
values (Pos Pred Value) and negative predicted values (Neg Pred
Value) shows a similar pattern for all land use/cover classes except
water class. Generally, the positive predicted values (Pos Pred Value)
for all land use/cover classes except the water class are lower than the
negative predicted values (Neg Pred Value). This means that from the
map user’s perspective, the ANN classifier is poor at identifying the
positive land use/cover classes.
The ANN classifier is also better at detecting the true negative
classes. A closer look at the confusion matrix shows high misclassification errors, which suggests significant spectral confusion
between the agriculture and bareland classes. As a result, the ANN
classifier has difficulty to separate agriculture and bareland areas.
Here, we see that poor performance of the ANN model is due to high
between predictor correlations. Be aware that the ANN model is
sensitive to high predictor collinearity
